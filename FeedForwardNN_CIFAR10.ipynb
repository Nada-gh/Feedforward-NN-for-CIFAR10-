{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you make sure that you are not overfitting in both of these cases?\n",
    "One of the most important decisions that you have to make when training a network is what loss function you need, given that this is the a classifier what loss function should you use? Even if you know the answer try out different loss functions and prove to yourself that certain loss functions do much better than others, if you can derive why you should use this loss fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in: int, n_out: int, bn: bool = True, p: float = 0., actn: torch.nn.Module = None, sequential : bool = False):\n",
    "    \"\"\"\n",
    "    Utility function that adds batch norm, dropout and linear layer \n",
    "    \n",
    "    Arguments : \n",
    "        n_in : Number of input neurons\n",
    "        n_out : Number of output neurons\n",
    "        bn : If there is a batch norm layer\n",
    "        p : Bathc norm dropout rate\n",
    "        act : Activation for the linear layer\n",
    "    \n",
    "    Returns : \n",
    "        List of batch norm, dropout and linear layer\n",
    "    \n",
    "    \"\"\"\n",
    "    layers = [torch.nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0:\n",
    "        layers.append(torch.nn.Dropout(p))\n",
    "    layers.append(torch.nn.Linear(n_in, n_out))\n",
    "    if actn is not None:\n",
    "        layers.append(actn)\n",
    "    if sequential :\n",
    "        return torch.nn.Sequential(layers)\n",
    "    else :\n",
    "        return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Dropout(p=0.5),\n",
       " Linear(in_features=100, out_features=500, bias=True),\n",
       " ReLU()]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#layer = bn_drop_lin(n_in = 100, n_out = 500, bn = True, p = 0.5, actn = nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.CIFAR10(root='./data', train=True, transform=transforms.ToTensor(),download=True)\n",
    "\n",
    "test_dataset = dsets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: MAKING DATASET ITERABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters/(len(train_dataset)/batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: CREATE MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 100\n",
    "n_out = 500\n",
    "bn = True \n",
    "p = 0.5\n",
    "actn = nn.ReLU()\n",
    "\n",
    "input_dim = 32*32*3\n",
    "hidden_dims = [100,500,1000,100]\n",
    "num_classes = 10\n",
    "bn = True\n",
    "drop = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim : int , hidden_dims : list, num_classes : int,bn : bool, drop: float,actn):\n",
    "        \"\"\"\n",
    "        Simple Feedforward Net that accpest variable amount of layers\n",
    "        \n",
    "        Arguments : \n",
    "            input_dim : Size of the input dimension\n",
    "            hidden_dims : List containing size of all hidden layers\n",
    "            num_classes : Number of classes\n",
    "            bn : If there is a batch norm layer\n",
    "            drop : dropout rate\n",
    "        \"\"\"\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        \n",
    "        layer_size = [input_dim] +hidden_dims +[num_classes]\n",
    "        \n",
    "        layers = []\n",
    "        for n_in, n_out in zip(layer_size[:-1], layer_size[1:]):\n",
    "            if n_out != hidden_dims :\n",
    "                #add ReLU for every layer except last one\n",
    "                layers += bn_drop_lin(n_in, n_out, bn, drop, actn)\n",
    "            else : \n",
    "                #don't add ReLU to last layer\n",
    "                layers += bn_drop_lin(n_in, n_out, bn, drop, None)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: INSTANTIATE MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = FeedforwardNeuralNetModel(input_dim, hidden_dims,num_classes)\n",
    "#  USE GPU FOR MODEL  \n",
    "\n",
    "#if torch.cuda.is_available():\n",
    " #   model.cuda()\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dims, num_classes, bn, drop,actn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNeuralNetModel(\n",
      "  (layers): Sequential(\n",
      "    (0): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Dropout(p=0.5)\n",
      "    (2): Linear(in_features=3072, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=100, out_features=500, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Dropout(p=0.5)\n",
      "    (10): Linear(in_features=500, out_features=1000, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): Dropout(p=0.5)\n",
      "    (14): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): Dropout(p=0.5)\n",
      "    (18): Linear(in_features=100, out_features=10, bias=True)\n",
      "    (19): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: INSTANTIATE LOSS CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: INSTANTIATE OPTIMIZER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.293675422668457. Accuracy: 14\n",
      "Iteration: 1000. Loss: 2.256018877029419. Accuracy: 16\n",
      "Iteration: 1500. Loss: 2.2233896255493164. Accuracy: 19\n",
      "Iteration: 2000. Loss: 2.1804070472717285. Accuracy: 20\n",
      "Iteration: 2500. Loss: 2.1367316246032715. Accuracy: 21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        \n",
    "        images = Variable(images.view(-1, 3*32*32))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                images = Variable(images.view(-1,3*32*32))\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
